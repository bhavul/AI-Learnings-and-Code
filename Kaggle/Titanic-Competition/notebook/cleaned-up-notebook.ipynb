{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named hyperdash",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9a5dbf52d80c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Input data files are available in the \"../input/\" directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhyperdash\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmonitor_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named hyperdash"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "from hyperdash import monitor_cell\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_of_NaN_rows(df):\n",
    "    return df.isnull().sum()\n",
    "\n",
    "def fill_NaN_values_for_numerical_column(df, colname):\n",
    "    df[colname] = df[colname].fillna(df[colname].mean())\n",
    "    return df\n",
    "\n",
    "def fill_NaN_values_for_categorical_column(df, colname, value):\n",
    "    df[colname] = df[colname].fillna(value)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a helper method from this now.\n",
    "def find_categorical_columns(df):\n",
    "    all_cols = df.columns\n",
    "    numeric_cols = df._get_numeric_data().columns\n",
    "    return set(all_cols) - set(numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make helper function here also\n",
    "def convert_categorical_column_to_integer_values(df):\n",
    "    df_numerical = df.copy()\n",
    "    for col in find_categorical_columns(df):\n",
    "        df_numerical[col] = df_numerical[col].astype('category')\n",
    "        df_numerical[col] = df_numerical[col].cat.codes\n",
    "    return df_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's list our helper functions we could make from logic used above.\n",
    "def convert_sigmoid_output_to_boolean_array(array, threshold):\n",
    "    array = array > threshold\n",
    "    return array\n",
    "\n",
    "def convert_boolean_array_to_binary_array(array):\n",
    "    array_binary = array.astype(int)\n",
    "    return array_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tensorflow model\n",
    "def model_generic(learning_rate, X_arg, Y_arg, num_of_epochs, hidden_units, threshold):\n",
    "    # 1. Placeholders to hold data\n",
    "    X = tf.placeholder(tf.float32, [X_arg.shape[0],None])\n",
    "    Y = tf.placeholder(tf.float32, [1, None])\n",
    "\n",
    "    # 2. Model. 2 layers NN. So, W1, b1, W2, b2.\n",
    "    # This is basically coding forward propagation formulaes\n",
    "    W1 = tf.Variable(tf.random_normal((hidden_units,X_arg.shape[0])))\n",
    "    b1 = tf.Variable(tf.zeros((hidden_units,1)))\n",
    "    Z1 = tf.matmul(W1,X) + b1\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "\n",
    "    W2 = tf.Variable(tf.random_normal((1, hidden_units)))\n",
    "    b2 = tf.Variable(tf.zeros((1,1)))\n",
    "    Z2 = tf.matmul(W2,A1) + b2\n",
    "    A2 = tf.nn.sigmoid(Z2)\n",
    "\n",
    "    # 3. Calculate cost\n",
    "    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=Z2, labels=Y)\n",
    "    cost_mean = tf.reduce_mean(cost)\n",
    "\n",
    "    # 4. Optimizer (Gradient Descent / AdamOptimizer ) - Using this line, tensorflow automatically does backpropagation\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost_mean)\n",
    "    \n",
    "    # 5. Accuracy methods\n",
    "    predicted_class = tf.greater(A2,threshold)\n",
    "    prediction_arr = tf.equal(predicted_class, tf.equal(Y,1.0))\n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction_arr, tf.float32))\n",
    "    \n",
    "    # 5. initialize variabls\n",
    "    session = tf.Session()\n",
    "    tf.set_random_seed(1)\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "    \n",
    "    # 6. Actual loop where learning happens\n",
    "    for i in range(num_of_epochs):\n",
    "        _, cost_mean_val, accuracy_val = session.run([optimizer, cost_mean, accuracy], feed_dict={X:X_arg, Y:Y_arg})\n",
    "        if i % 100 == 0:\n",
    "            print(\"i:\",i,\", cost : \",cost_mean_val,\", training accuracy : \",accuracy_val)\n",
    "            \n",
    "    return session.run([W1,b1,W2,b2,A2,Y,accuracy],feed_dict={X:X_arg, Y:Y_arg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Ref : https://stackoverflow.com/questions/32109319/how-to-implement-the-relu-function-in-numpy\n",
    "# Ref : https://stackoverflow.com/questions/3985619/how-to-calculate-a-logistic-sigmoid-function-in-python\n",
    "def predict(W1,b1,W2,b2,X):\n",
    "    \n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = np.maximum(Z1, 0, Z1)\n",
    "    \n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = 1 / (1 + np.exp(-Z2))\n",
    "    return A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper exercise which does the whole thing for any training dataframe given \n",
    "def execute_steps_for_titanic(columns_to_use, output_file_name, learning_rate=0.01, num_of_epochs=3000, hidden_units=50, threshold_for_output=0.5, ):\n",
    "    # read data\n",
    "    training_df_orig = pd.read_csv(\"../input/train.csv\")\n",
    "    testing_df_orig = pd.read_csv(\"../input/test.csv\")\n",
    "    # get X and Y separated\n",
    "    train_df_Y = training_df_orig['Survived']\n",
    "    train_df_X = training_df_orig[columns_to_use]\n",
    "    test_df_X = testing_df_orig[columns_to_use]\n",
    "    # fix missing data\n",
    "    categorical_columns = find_categorical_columns(train_df_X)\n",
    "    replace_values_dict = {'Embarked':'S', 'Cabin':'UNKNOWN'}\n",
    "    for col in columns_to_use:\n",
    "        num_of_NaN_rows = get_num_of_NaN_rows(train_df_X)[col]\n",
    "        num_of_NaN_rows_test = get_num_of_NaN_rows(test_df_X)[col]\n",
    "        if(num_of_NaN_rows > 0):\n",
    "            print(\"Filling NaN values for column:\",col)\n",
    "            if col not in categorical_columns:\n",
    "                train_df_X[col] = train_df_X[col].fillna(train_df_X[col].mean())\n",
    "            else:\n",
    "                train_df_X[col] = train_df_X[col].fillna(replace_values_dict[col])\n",
    "        if(num_of_NaN_rows_test > 0):\n",
    "            print(\"Filling NaN values for column:\",col,\" in test data\")\n",
    "            if col not in categorical_columns:\n",
    "                test_df_X[col] = test_df_X[col].fillna(test_df_X[col].mean())\n",
    "            else:\n",
    "                test_df_X[col] = test_df_X[col].fillna(replace_values_dict[col])\n",
    "    print(\"Fixed NaN values in training and testing data.\")\n",
    "    # convert categorical to numerical data\n",
    "    train_df_X_num = convert_categorical_column_to_integer_values(train_df_X)\n",
    "    test_df_X_num = convert_categorical_column_to_integer_values(test_df_X)\n",
    "    # Get numpy arrays for this data\n",
    "    train_X = train_df_X_num.as_matrix()\n",
    "    test_X = test_df_X_num.as_matrix()\n",
    "    train_Y = train_df_Y.as_matrix()\n",
    "    # fix rank-1 array created\n",
    "    train_Y = train_Y[:,np.newaxis]\n",
    "    # call model and get values \n",
    "    W1,b1,W2,b2,A2,Y,final_tr_accuracy = model_generic(learning_rate, train_X.T, train_Y.T, num_of_epochs, hidden_units, threshold_for_output)\n",
    "    print(\"Final training accuracy : \",final_tr_accuracy)\n",
    "    # get prediction and save it to output file\n",
    "    prediction = predict(W1,b1,W2,b2,test_X.T)\n",
    "    # if prediction value > threshold, then set as True, else as False\n",
    "    prediction = prediction > threshold_for_output\n",
    "    # Convert the True/False array to a 0 , 1 array\n",
    "    prediction = prediction.astype(int)\n",
    "    # Convert back to dataframe and give the column name as 'Survived'\n",
    "    prediction_df = pd.DataFrame(data=prediction.T, columns=['Survived'])\n",
    "    # Make a final data frame of the required output and output to csv\n",
    "    final_df = pd.concat([testing_df_orig['PassengerId'], prediction_df], axis=1)\n",
    "    final_file_name = output_file_name+\"_tr_acc_\"+\"{0:.2f}\".format(final_tr_accuracy)+\"_prediction.csv\"\n",
    "    final_df.to_csv(final_file_name, index=False)\n",
    "    print(\"Done.\")\n",
    "    return final_file_name, final_tr_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
