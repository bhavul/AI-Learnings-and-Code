{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv1D VS Conv2D vs Conv3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically extracted from [this stackoverflow answer](https://stackoverflow.com/questions/42883547/what-do-you-mean-by-1d-2d-and-3d-convolutions-in-cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv1D\n",
    "\n",
    "- input = [W], filter = [k], output = [W]\n",
    "- output-shape is 1D array\n",
    "- convolutional direction is one way. Just forward, for example\n",
    "\n",
    "This is used for graph smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![conv1d-explanation](./conv1d.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  3.  3.  3.  2.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "ones_1d = np.ones(5)\n",
    "weight_1d = np.ones(3)\n",
    "strides_1d = 1\n",
    "\n",
    "in_1d = tf.constant(ones_1d, dtype=tf.float32)\n",
    "filter_1d = tf.constant(weight_1d, dtype=tf.float32)\n",
    "\n",
    "in_width = int(in_1d.shape[0])\n",
    "filter_width = int(filter_1d.shape[0])\n",
    "\n",
    "input_1d   = tf.reshape(in_1d, [1, in_width, 1])\n",
    "kernel_1d = tf.reshape(filter_1d, [filter_width, 1, 1])\n",
    "output_1d = tf.squeeze(tf.nn.conv1d(input_1d, kernel_1d, strides_1d, padding='SAME'))\n",
    "print(sess.run(output_1d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2D\n",
    "\n",
    "- 2-direction (x,y) to calculate conv\n",
    "- output-shape is 2D Matrix\n",
    "- input = [W, H], filter = [k,k] output = [W,H]\n",
    "\n",
    "Toy example is analogous to above one, just that everything's in 2D now. Output HAS TO be 2D array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2d](https://i.stack.imgur.com/hvMaU.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.  6.  6.  6.  4.]\n",
      " [ 6.  9.  9.  9.  6.]\n",
      " [ 6.  9.  9.  9.  6.]\n",
      " [ 6.  9.  9.  9.  6.]\n",
      " [ 4.  6.  6.  6.  4.]]\n"
     ]
    }
   ],
   "source": [
    "ones_2d = np.ones((5,5))\n",
    "weight_2d = np.ones((3,3))\n",
    "strides_2d = [1, 1, 1, 1]\n",
    "\n",
    "in_2d = tf.constant(ones_2d, dtype=tf.float32)\n",
    "filter_2d = tf.constant(weight_2d, dtype=tf.float32)\n",
    "\n",
    "in_width = int(in_2d.shape[0])\n",
    "in_height = int(in_2d.shape[1])\n",
    "\n",
    "filter_width = int(filter_2d.shape[0])\n",
    "filter_height = int(filter_2d.shape[1])\n",
    "\n",
    "input_2d   = tf.reshape(in_2d, [1, in_height, in_width, 1])\n",
    "kernel_2d = tf.reshape(filter_2d, [filter_height, filter_width, 1, 1])\n",
    "\n",
    "output_2d = tf.squeeze(tf.nn.conv2d(input_2d, kernel_2d, strides=strides_2d, padding='SAME'))\n",
    "print(sess.run(output_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv3D\n",
    "\n",
    "- 3-direction (x,y,z) to calcuate conv\n",
    "- output-shape is 3D Volume\n",
    "- input = [W,H,L], filter = [k,k,d] output = [W,H,M]\n",
    "- d < L is important! for making volume output\n",
    "\n",
    "Remember, that input dimensions could be bigger, but output HAS TO BE 3D Volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3d](https://i.stack.imgur.com/IvDQP.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  8.  12.  12.  12.   8.]\n",
      "  [ 12.  18.  18.  18.  12.]\n",
      "  [ 12.  18.  18.  18.  12.]\n",
      "  [ 12.  18.  18.  18.  12.]\n",
      "  [  8.  12.  12.  12.   8.]]\n",
      "\n",
      " [[ 12.  18.  18.  18.  12.]\n",
      "  [ 18.  27.  27.  27.  18.]\n",
      "  [ 18.  27.  27.  27.  18.]\n",
      "  [ 18.  27.  27.  27.  18.]\n",
      "  [ 12.  18.  18.  18.  12.]]\n",
      "\n",
      " [[ 12.  18.  18.  18.  12.]\n",
      "  [ 18.  27.  27.  27.  18.]\n",
      "  [ 18.  27.  27.  27.  18.]\n",
      "  [ 18.  27.  27.  27.  18.]\n",
      "  [ 12.  18.  18.  18.  12.]]\n",
      "\n",
      " [[ 12.  18.  18.  18.  12.]\n",
      "  [ 18.  27.  27.  27.  18.]\n",
      "  [ 18.  27.  27.  27.  18.]\n",
      "  [ 18.  27.  27.  27.  18.]\n",
      "  [ 12.  18.  18.  18.  12.]]\n",
      "\n",
      " [[  8.  12.  12.  12.   8.]\n",
      "  [ 12.  18.  18.  18.  12.]\n",
      "  [ 12.  18.  18.  18.  12.]\n",
      "  [ 12.  18.  18.  18.  12.]\n",
      "  [  8.  12.  12.  12.   8.]]]\n"
     ]
    }
   ],
   "source": [
    "ones_3d = np.ones((5,5,5))\n",
    "weight_3d = np.ones((3,3,3))\n",
    "strides_3d = [1, 1, 1, 1, 1]\n",
    "\n",
    "in_3d = tf.constant(ones_3d, dtype=tf.float32)\n",
    "filter_3d = tf.constant(weight_3d, dtype=tf.float32)\n",
    "\n",
    "in_width = int(in_3d.shape[0])\n",
    "in_height = int(in_3d.shape[1])\n",
    "in_depth = int(in_3d.shape[2])\n",
    "\n",
    "filter_width = int(filter_3d.shape[0])\n",
    "filter_height = int(filter_3d.shape[1])\n",
    "filter_depth = int(filter_3d.shape[2])\n",
    "\n",
    "input_3d   = tf.reshape(in_3d, [1, in_depth, in_height, in_depth, 1])\n",
    "kernel_3d = tf.reshape(filter_3d, [filter_depth, filter_height, filter_width, 1, 1])\n",
    "\n",
    "output_3d = tf.squeeze(tf.nn.conv3d(input_3d, kernel_3d, strides=strides_3d, padding='SAME'))\n",
    "print(sess.run(output_3d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Conv2D for RGB Images instead of Conv3D\n",
    "\n",
    "Now, when we actually work with RGB Images, we often use Conv2D only, instead of Conv3D. \n",
    "Remember that each filter gives one 2d matrix only. It is number of filters that create the depth! \n",
    "\n",
    "Frankly, theoritically you could use Conv3D as well for convolving. However, that increases computation complexity a bit. And people have just been following conv2D because it seems to work well.\n",
    "\n",
    "Moreover, principal reasons.\n",
    "2D convolution means to convolve two dimension data like picture or image, which has height and width. It is not for RGB channel; it is for height and width.Thus 3D convolution is for three dimensional data, like cube which has height, width and depth, or video which has height, width and time.\n",
    "\n",
    "Further, In an RGB image, the image is a∗b∗3a∗b∗3 and the convolutional filters are cc∗d∗3∗d∗3. Since their third dimensions are equal, there is no need to convolve along that axis. You only convolve along the first two axes, making it a 2D convolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
