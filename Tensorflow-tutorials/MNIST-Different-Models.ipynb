{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Different Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from hyperdash import monitor_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets(\"./data/MNIST/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow.examples.tutorials.mnist.input_data' from '/Users/bhavul.g/.virtualenvs/ailearn/lib/python3.5/site-packages/tensorflow/examples/tutorials/mnist/input_data.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\n",
      "\"\"\"Functions for downloading and reading MNIST data.\"\"\"\n",
      "from __future__ import absolute_import\n",
      "from __future__ import division\n",
      "from __future__ import print_function\n",
      "\n",
      "import gzip\n",
      "import os\n",
      "import tempfile\n",
      "\n",
      "import numpy\n",
      "from six.moves import urllib\n",
      "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
      "import tensorflow as tf\n",
      "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# So it is a python file essentially. A module. \n",
    "\n",
    "import inspect\n",
    "src = inspect.getsource(input_data)\n",
    "print(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def read_data_sets(train_dir,\n",
      "                   fake_data=False,\n",
      "                   one_hot=False,\n",
      "                   dtype=dtypes.float32,\n",
      "                   reshape=True,\n",
      "                   validation_size=5000,\n",
      "                   seed=None):\n",
      "  if fake_data:\n",
      "\n",
      "    def fake():\n",
      "      return DataSet(\n",
      "          [], [], fake_data=True, one_hot=one_hot, dtype=dtype, seed=seed)\n",
      "\n",
      "    train = fake()\n",
      "    validation = fake()\n",
      "    test = fake()\n",
      "    return base.Datasets(train=train, validation=validation, test=test)\n",
      "\n",
      "  TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'\n",
      "  TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'\n",
      "  TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n",
      "  TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n",
      "\n",
      "  local_file = base.maybe_download(TRAIN_IMAGES, train_dir,\n",
      "                                   SOURCE_URL + TRAIN_IMAGES)\n",
      "  with open(local_file, 'rb') as f:\n",
      "    train_images = extract_images(f)\n",
      "\n",
      "  local_file = base.maybe_download(TRAIN_LABELS, train_dir,\n",
      "                                   SOURCE_URL + TRAIN_LABELS)\n",
      "  with open(local_file, 'rb') as f:\n",
      "    train_labels = extract_labels(f, one_hot=one_hot)\n",
      "\n",
      "  local_file = base.maybe_download(TEST_IMAGES, train_dir,\n",
      "                                   SOURCE_URL + TEST_IMAGES)\n",
      "  with open(local_file, 'rb') as f:\n",
      "    test_images = extract_images(f)\n",
      "\n",
      "  local_file = base.maybe_download(TEST_LABELS, train_dir,\n",
      "                                   SOURCE_URL + TEST_LABELS)\n",
      "  with open(local_file, 'rb') as f:\n",
      "    test_labels = extract_labels(f, one_hot=one_hot)\n",
      "\n",
      "  if not 0 <= validation_size <= len(train_images):\n",
      "    raise ValueError(\n",
      "        'Validation size should be between 0 and {}. Received: {}.'\n",
      "        .format(len(train_images), validation_size))\n",
      "\n",
      "  validation_images = train_images[:validation_size]\n",
      "  validation_labels = train_labels[:validation_size]\n",
      "  train_images = train_images[validation_size:]\n",
      "  train_labels = train_labels[validation_size:]\n",
      "\n",
      "  \n",
      "  options = dict(dtype=dtype, reshape=reshape, seed=seed)\n",
      "  \n",
      "  train = DataSet(train_images, train_labels, **options)\n",
      "  validation = DataSet(validation_images, validation_labels, **options)\n",
      "  test = DataSet(test_images, test_labels, **options)\n",
      "  \n",
      "  return base.Datasets(train=train, validation=validation, test=test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Okay, so actual logic must be in read_data_sets function.\n",
    "\n",
    "srcMethod = inspect.getsource(input_data.read_data_sets)\n",
    "print(srcMethod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def extract_labels(f, one_hot=False, num_classes=10):\n",
      "  \"\"\"Extract the labels into a 1D uint8 numpy array [index].\n",
      "\n",
      "  Args:\n",
      "    f: A file object that can be passed into a gzip reader.\n",
      "    one_hot: Does one hot encoding for the result.\n",
      "    num_classes: Number of classes for the one hot encoding.\n",
      "\n",
      "  Returns:\n",
      "    labels: a 1D uint8 numpy array.\n",
      "\n",
      "  Raises:\n",
      "    ValueError: If the bystream doesn't start with 2049.\n",
      "  \"\"\"\n",
      "  print('Extracting', f.name)\n",
      "  with gzip.GzipFile(fileobj=f) as bytestream:\n",
      "    magic = _read32(bytestream)\n",
      "    if magic != 2049:\n",
      "      raise ValueError('Invalid magic number %d in MNIST label file: %s' %\n",
      "                       (magic, f.name))\n",
      "    num_items = _read32(bytestream)\n",
      "    buf = bytestream.read(num_items)\n",
      "    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
      "    if one_hot:\n",
      "      return dense_to_one_hot(labels, num_classes)\n",
      "    return labels\n",
      "\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "def extract_images(f):\n",
      "  \"\"\"Extract the images into a 4D uint8 numpy array [index, y, x, depth].\n",
      "\n",
      "  Args:\n",
      "    f: A file object that can be passed into a gzip reader.\n",
      "\n",
      "  Returns:\n",
      "    data: A 4D uint8 numpy array [index, y, x, depth].\n",
      "\n",
      "  Raises:\n",
      "    ValueError: If the bytestream does not start with 2051.\n",
      "\n",
      "  \"\"\"\n",
      "  print('Extracting', f.name)\n",
      "  with gzip.GzipFile(fileobj=f) as bytestream:\n",
      "    magic = _read32(bytestream)\n",
      "    if magic != 2051:\n",
      "      raise ValueError('Invalid magic number %d in MNIST image file: %s' %\n",
      "                       (magic, f.name))\n",
      "    num_images = _read32(bytestream)\n",
      "    rows = _read32(bytestream)\n",
      "    cols = _read32(bytestream)\n",
      "    buf = bytestream.read(rows * cols * num_images)\n",
      "    data = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
      "    data = data.reshape(num_images, rows, cols, 1)\n",
      "    return data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The above code seems very intuitive. Just extract_images and extract_labels could be checked\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "\n",
    "src_extract_labels = inspect.getsource(mnist.extract_labels)\n",
    "src_extract_images = inspect.getsource(mnist.extract_images)\n",
    "\n",
    "print(src_extract_labels)\n",
    "print(\"\\n------------------------------------------------------------------\\n\")\n",
    "print(src_extract_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x11dba5208>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x11dba51d0>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x11dba5278>)\n"
     ]
    }
   ],
   "source": [
    "# Cool. Easy enough to understand. Let's check what is in data now.\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n"
     ]
    }
   ],
   "source": [
    "print(len(data.train.images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.38039219  0.37647063\n",
      "  0.3019608   0.46274513  0.2392157   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.35294119  0.5411765\n",
      "  0.92156869  0.92156869  0.92156869  0.92156869  0.92156869  0.92156869\n",
      "  0.98431379  0.98431379  0.97254908  0.99607849  0.96078438  0.92156869\n",
      "  0.74509805  0.08235294  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.54901963  0.98431379  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.99607849  0.99607849  0.99607849  0.74117649  0.09019608\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.88627458  0.99607849  0.81568635\n",
      "  0.78039223  0.78039223  0.78039223  0.78039223  0.54509807  0.2392157\n",
      "  0.2392157   0.2392157   0.2392157   0.2392157   0.50196081  0.8705883\n",
      "  0.99607849  0.99607849  0.74117649  0.08235294  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.14901961  0.32156864  0.0509804   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.13333334  0.83529419  0.99607849  0.99607849  0.45098042  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.32941177  0.99607849  0.99607849  0.91764712  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.32941177  0.99607849  0.99607849  0.91764712  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.41568631  0.6156863   0.99607849  0.99607849  0.95294124  0.20000002\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.09803922  0.45882356  0.89411771\n",
      "  0.89411771  0.89411771  0.99215692  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.94117653  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.26666668  0.4666667   0.86274517\n",
      "  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.99607849  0.99607849  0.55686277  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.14509805  0.73333335  0.99215692\n",
      "  0.99607849  0.99607849  0.99607849  0.87450987  0.80784321  0.80784321\n",
      "  0.29411766  0.26666668  0.84313732  0.99607849  0.99607849  0.45882356\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.44313729\n",
      "  0.8588236   0.99607849  0.94901967  0.89019614  0.45098042  0.34901962\n",
      "  0.12156864  0.          0.          0.          0.          0.7843138\n",
      "  0.99607849  0.9450981   0.16078432  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.66274512  0.99607849  0.6901961   0.24313727  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.18823531\n",
      "  0.90588242  0.99607849  0.91764712  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.07058824  0.48627454  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.32941177  0.99607849  0.99607849  0.65098041  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.54509807  0.99607849  0.9333334   0.22352943  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.82352948  0.98039222  0.99607849  0.65882355  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.94901967  0.99607849  0.93725497  0.22352943  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.34901962  0.98431379  0.9450981   0.33725491  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01960784  0.80784321  0.96470594  0.6156863   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.01568628  0.45882356  0.27058825  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Check one example\n",
    "\n",
    "print(data.train.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n"
     ]
    }
   ],
   "source": [
    "# What was the dimensions?\n",
    "\n",
    "print(data.train.images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The img above is meant to be  [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCIrwA0ERfiAowg8ERfiBoAg/ENT/AyErW1pw/s8cAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1054cd550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AAh. So, must be 28 * 28. Let's reshape and try to plot then. \n",
    "# https://matplotlib.org/users/image_tutorial.html   ------ This tells how we could plot it.\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "imgplot = plt.imshow(data.train.images[0].reshape((28,28)))\n",
    "print(\"The img above is meant to be \",data.train.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The img above is meant to be  [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngiL8QFCEHwiK8ANBEX4gKMIPBPX/EhqoeSQulYEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x129f44a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's do another one.\n",
    "imgplot2 = plt.imshow(data.train.images[1].reshape((28,28)))\n",
    "print(\"The img above is meant to be \",data.train.labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of training data X :  (55000, 784)\n",
      "Dimensions of training data Y :  (55000, 10)\n",
      "Dimensions of training data X :  (5000, 784)\n",
      "Dimensions of training data Y :  (5000, 10)\n",
      "Dimensions of training data X :  (10000, 784)\n",
      "Dimensions of training data Y :  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Let's check dimensions\n",
    "\n",
    "print(\"Dimensions of training data X : \",data.train.images.shape)\n",
    "print(\"Dimensions of training data Y : \",data.train.labels.shape)\n",
    "print(\"Dimensions of training data X : \",data.validation.images.shape)\n",
    "print(\"Dimensions of training data Y : \",data.validation.labels.shape)\n",
    "print(\"Dimensions of training data X : \",data.test.images.shape)\n",
    "print(\"Dimensions of training data Y : \",data.test.labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So, every row here represents an image / one_hot_label. Instead of a column. This is where it differs from the deeplearning.ai course**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Mini-Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at  0 th iteration is :  0.586658\n",
      "cost at  5 th iteration is :  0.391887\n",
      "cost at  10 th iteration is :  0.342949\n",
      "cost at  15 th iteration is :  0.321183\n"
     ]
    }
   ],
   "source": [
    "# code to add\n",
    "X1 = tf.placeholder(tf.float32, [784, None])\n",
    "Y1 = tf.placeholder(tf.float32, [10, None])\n",
    "\n",
    "# Model is simple W*X + b. \n",
    "# IMPORTANT : If say we're doing for 55000 examples. W would be 10x784, X is 784x55000,  b should ideally be 10,55000. But 55000 is not known from before.\n",
    "# So, the point is, 'Variable' needs initialization which can not have 'None'. So, how do we fix this?\n",
    "# Just keep it as dimension - (10,1). Python, numpy, tf ---- let them do BROADCASTING :-)\n",
    "\n",
    "W1 = tf.Variable(tf.zeros((10,784)))\n",
    "b1 = tf.Variable(tf.zeros((10,1)))           \n",
    "\n",
    "Z1 = tf.matmul(W1,X1) + b1\n",
    "\n",
    "# We'll do a softmax for this so that ever number comes b/w 0 and 1. Together they should sum up to 1. Normalization.\n",
    "Y1_pred_softmax = tf.nn.softmax(Z1)\n",
    "Y1_pred_sigmoid = tf.nn.sigmoid(Z1)\n",
    "\n",
    "\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=Z1, labels=Y1)\n",
    "# Reduce cost to just one number. \n",
    "cost_mean = tf.reduce_mean(cost) \n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost_mean)\n",
    "\n",
    "session = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "\n",
    "# 1000 epochs \n",
    "for i in range(20):\n",
    "    session.run(optimizer, feed_dict={X1:data.train.images.T, Y1:data.train.labels.T})\n",
    "    if i%5 == 0:\n",
    "        print(\"cost at \",i,\"th iteration is : \",session.run(cost_mean, feed_dict={X1:data.train.images.T, Y1:data.train.labels.T}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without mini-batches, more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at  0 th iteration is :  0.586658\n",
      "cost at  5 th iteration is :  0.391887\n",
      "cost at  10 th iteration is :  0.342949\n",
      "cost at  15 th iteration is :  0.321183\n",
      "cost at  20 th iteration is :  0.307678\n",
      "cost at  25 th iteration is :  0.297502\n",
      "cost at  30 th iteration is :  0.288973\n",
      "cost at  35 th iteration is :  0.281426\n",
      "cost at  40 th iteration is :  0.274557\n",
      "cost at  45 th iteration is :  0.268211\n",
      "cost at  50 th iteration is :  0.262309\n",
      "cost at  55 th iteration is :  0.256788\n",
      "cost at  60 th iteration is :  0.251614\n",
      "cost at  65 th iteration is :  0.246754\n",
      "cost at  70 th iteration is :  0.242181\n",
      "cost at  75 th iteration is :  0.237871\n",
      "cost at  80 th iteration is :  0.233806\n",
      "cost at  85 th iteration is :  0.229963\n",
      "cost at  90 th iteration is :  0.226329\n",
      "cost at  95 th iteration is :  0.222887\n"
     ]
    }
   ],
   "source": [
    "# code to add\n",
    "X1 = tf.placeholder(tf.float32, [784, None])\n",
    "Y1 = tf.placeholder(tf.float32, [10, None])\n",
    "\n",
    "# Model is simple W*X + b. \n",
    "# IMPORTANT : If say we're doing for 55000 examples. W would be 10x784, X is 784x55000,  b should ideally be 10,55000. But 55000 is not known from before.\n",
    "# So, the point is, 'Variable' needs initialization which can not have 'None'. So, how do we fix this?\n",
    "# Just keep it as dimension - (10,1). Python, numpy, tf ---- let them do BROADCASTING :-)\n",
    "\n",
    "W1 = tf.Variable(tf.zeros((10,784)))\n",
    "b1 = tf.Variable(tf.zeros((10,1)))           \n",
    "\n",
    "Z1 = tf.matmul(W1,X1) + b1\n",
    "\n",
    "# We'll do a softmax for this so that ever number comes b/w 0 and 1. Together they should sum up to 1. Normalization.\n",
    "Y1_pred_softmax = tf.nn.softmax(Z1)\n",
    "Y1_pred_sigmoid = tf.nn.sigmoid(Z1)\n",
    "\n",
    "\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=Z1, labels=Y1)\n",
    "# Reduce cost to just one number. \n",
    "cost_mean = tf.reduce_mean(cost) \n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost_mean)\n",
    "\n",
    "session = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "\n",
    "# 1000 epochs \n",
    "for i in range(100):\n",
    "    session.run(optimizer, feed_dict={X1:data.train.images.T, Y1:data.train.labels.T})\n",
    "    if i%5 == 0:\n",
    "        print(\"cost at \",i,\"th iteration is : \",session.run(cost_mean, feed_dict={X1:data.train.images.T, Y1:data.train.labels.T}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at  0 th iteration is :  0.257947\n",
      "cost at  5 th iteration is :  0.15263\n",
      "cost at  10 th iteration is :  0.128502\n",
      "cost at  15 th iteration is :  0.116752\n"
     ]
    }
   ],
   "source": [
    "# code to add\n",
    "X1 = tf.placeholder(tf.float32, [784, None])\n",
    "Y1 = tf.placeholder(tf.float32, [10, None])\n",
    "\n",
    "# Model is simple W*X + b. \n",
    "# IMPORTANT : If say we're doing for 55000 examples. W would be 10x784, X is 784x55000,  b should ideally be 10,55000. But 55000 is not known from before.\n",
    "# So, the point is, 'Variable' needs initialization which can not have 'None'. So, how do we fix this?\n",
    "# Just keep it as dimension - (10,1). Python, numpy, tf ---- let them do BROADCASTING :-)\n",
    "\n",
    "W1 = tf.Variable(tf.zeros((10,784)))\n",
    "b1 = tf.Variable(tf.zeros((10,1)))           \n",
    "\n",
    "Z1 = tf.matmul(W1,X1) + b1\n",
    "\n",
    "# We'll do a softmax for this so that ever number comes b/w 0 and 1. Together they should sum up to 1. Normalization.\n",
    "Y1_pred_softmax = tf.nn.softmax(Z1)\n",
    "Y1_pred_sigmoid = tf.nn.sigmoid(Z1)\n",
    "\n",
    "\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=Z1, labels=Y1)\n",
    "# Reduce cost to just one number. \n",
    "cost_mean = tf.reduce_mean(cost) \n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost_mean)\n",
    "\n",
    "session = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "\n",
    "# mini-batches of 1000 (so 55 mini-batches)\n",
    "for i in range(20):\n",
    "    for t in range(55):\n",
    "        X_batch, Y_batch = data.train.next_batch(1000)\n",
    "        session.run(optimizer, feed_dict={X1:X_batch.T, Y1:Y_batch.T})\n",
    "    if i%5 == 0:\n",
    "        print(\"cost at \",i,\"th iteration is : \",session.run(cost_mean, feed_dict={X1:data.train.images.T, Y1:data.train.labels.T}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Adam instead of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at  0 th iteration is :  0.14534\n",
      "cost at  5 th iteration is :  0.0687919\n",
      "cost at  10 th iteration is :  0.0697996\n",
      "cost at  15 th iteration is :  0.0731332\n"
     ]
    }
   ],
   "source": [
    "# code to add\n",
    "X1 = tf.placeholder(tf.float32, [784, None])\n",
    "Y1 = tf.placeholder(tf.float32, [10, None])\n",
    "\n",
    "# Model is simple W*X + b. \n",
    "# IMPORTANT : If say we're doing for 55000 examples. W would be 10x784, X is 784x55000,  b should ideally be 10,55000. But 55000 is not known from before.\n",
    "# So, the point is, 'Variable' needs initialization which can not have 'None'. So, how do we fix this?\n",
    "# Just keep it as dimension - (10,1). Python, numpy, tf ---- let them do BROADCASTING :-)\n",
    "\n",
    "W1 = tf.Variable(tf.zeros((10,784)))\n",
    "b1 = tf.Variable(tf.zeros((10,1)))           \n",
    "\n",
    "Z1 = tf.matmul(W1,X1) + b1\n",
    "\n",
    "# We'll do a softmax for this so that ever number comes b/w 0 and 1. Together they should sum up to 1. Normalization.\n",
    "Y1_pred_softmax = tf.nn.softmax(Z1)\n",
    "Y1_pred_sigmoid = tf.nn.sigmoid(Z1)\n",
    "\n",
    "\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=Z1, labels=Y1)\n",
    "# Reduce cost to just one number. \n",
    "cost_mean = tf.reduce_mean(cost) \n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost_mean)\n",
    "\n",
    "session = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "\n",
    "# mini-batches of 1000 (so 55 mini-batches)\n",
    "for i in range(25):\n",
    "    for t in range(55):\n",
    "        X_batch, Y_batch = data.train.next_batch(1000)\n",
    "        session.run(optimizer, feed_dict={X1:X_batch.T, Y1:Y_batch.T})\n",
    "    if i%5 == 0:\n",
    "        print(\"cost at \",i,\"th iteration is : \",session.run(cost_mean, feed_dict={X1:data.train.images.T, Y1:data.train.labels.T}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- Mini-batch obviously works better!\n",
    "- AdamOptizer is damn fast. But obviously, now your cost could increase/decrease a bit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at  0 th iteration is :  25.2978 [  10 1000]\n",
      "cost at  5 th iteration is :  6.38793 [  10 1000]\n",
      "cost at  10 th iteration is :  4.77624 [  10 1000]\n",
      "cost at  15 th iteration is :  3.20396 [  10 1000]\n",
      "cost at  20 th iteration is :  2.08947 [  10 1000]\n",
      "cost at  25 th iteration is :  1.65479 [  10 1000]\n",
      "cost at  30 th iteration is :  1.39821 [  10 1000]\n",
      "cost at  35 th iteration is :  1.04432 [  10 1000]\n",
      "cost at  40 th iteration is :  1.07579 [  10 1000]\n",
      "cost at  45 th iteration is :  0.855806 [  10 1000]\n",
      "cost at  50 th iteration is :  0.753868 [  10 1000]\n",
      "cost at  55 th iteration is :  0.728713 [  10 1000]\n",
      "cost at  60 th iteration is :  0.766429 [  10 1000]\n",
      "cost at  65 th iteration is :  0.682939 [  10 1000]\n",
      "cost at  70 th iteration is :  0.66986 [  10 1000]\n",
      "cost at  75 th iteration is :  0.574747 [  10 1000]\n",
      "cost at  80 th iteration is :  0.609634 [  10 1000]\n",
      "cost at  85 th iteration is :  0.672206 [  10 1000]\n",
      "cost at  90 th iteration is :  0.530539 [  10 1000]\n",
      "cost at  95 th iteration is :  0.485046 [  10 1000]\n",
      "cost at  100 th iteration is :  0.506577 [  10 1000]\n",
      "cost at  105 th iteration is :  0.505892 [  10 1000]\n",
      "cost at  110 th iteration is :  0.486161 [  10 1000]\n",
      "cost at  115 th iteration is :  0.472352 [  10 1000]\n",
      "cost at  120 th iteration is :  0.407021 [  10 1000]\n",
      "cost at  125 th iteration is :  0.445935 [  10 1000]\n",
      "cost at  130 th iteration is :  0.426024 [  10 1000]\n",
      "cost at  135 th iteration is :  0.422483 [  10 1000]\n",
      "cost at  140 th iteration is :  0.395961 [  10 1000]\n",
      "cost at  145 th iteration is :  0.357248 [  10 1000]\n",
      "cost at  150 th iteration is :  0.42126 [  10 1000]\n",
      "cost at  155 th iteration is :  0.36864 [  10 1000]\n",
      "cost at  160 th iteration is :  0.330363 [  10 1000]\n",
      "cost at  165 th iteration is :  0.338419 [  10 1000]\n",
      "cost at  170 th iteration is :  0.3744 [  10 1000]\n",
      "cost at  175 th iteration is :  0.337232 [  10 1000]\n",
      "cost at  180 th iteration is :  0.357804 [  10 1000]\n",
      "cost at  185 th iteration is :  0.30782 [  10 1000]\n",
      "cost at  190 th iteration is :  0.273085 [  10 1000]\n",
      "cost at  195 th iteration is :  0.271 [  10 1000]\n",
      "cost at  200 th iteration is :  0.304198 [  10 1000]\n",
      "cost at  205 th iteration is :  0.279718 [  10 1000]\n",
      "cost at  210 th iteration is :  0.239448 [  10 1000]\n",
      "cost at  215 th iteration is :  0.308647 [  10 1000]\n",
      "cost at  220 th iteration is :  0.255253 [  10 1000]\n",
      "cost at  225 th iteration is :  0.27422 [  10 1000]\n",
      "cost at  230 th iteration is :  0.242069 [  10 1000]\n",
      "cost at  235 th iteration is :  0.233677 [  10 1000]\n",
      "cost at  240 th iteration is :  0.194894 [  10 1000]\n",
      "cost at  245 th iteration is :  0.272318 [  10 1000]\n",
      "cost at  250 th iteration is :  0.213193 [  10 1000]\n",
      "cost at  255 th iteration is :  0.241515 [  10 1000]\n",
      "cost at  260 th iteration is :  0.275477 [  10 1000]\n",
      "cost at  265 th iteration is :  0.235704 [  10 1000]\n",
      "cost at  270 th iteration is :  0.186798 [  10 1000]\n",
      "cost at  275 th iteration is :  0.180255 [  10 1000]\n",
      "cost at  280 th iteration is :  0.177308 [  10 1000]\n",
      "cost at  285 th iteration is :  0.190613 [  10 1000]\n",
      "cost at  290 th iteration is :  0.199745 [  10 1000]\n",
      "cost at  295 th iteration is :  0.209214 [  10 1000]\n",
      "cost at  300 th iteration is :  0.193735 [  10 1000]\n",
      "cost at  305 th iteration is :  0.174923 [  10 1000]\n",
      "cost at  310 th iteration is :  0.160089 [  10 1000]\n",
      "cost at  315 th iteration is :  0.191572 [  10 1000]\n",
      "cost at  320 th iteration is :  0.229769 [  10 1000]\n",
      "cost at  325 th iteration is :  0.165144 [  10 1000]\n",
      "cost at  330 th iteration is :  0.15846 [  10 1000]\n",
      "cost at  335 th iteration is :  0.154142 [  10 1000]\n",
      "cost at  340 th iteration is :  0.184396 [  10 1000]\n",
      "cost at  345 th iteration is :  0.130729 [  10 1000]\n",
      "cost at  350 th iteration is :  0.152401 [  10 1000]\n",
      "cost at  355 th iteration is :  0.184966 [  10 1000]\n",
      "cost at  360 th iteration is :  0.17287 [  10 1000]\n",
      "cost at  365 th iteration is :  0.168669 [  10 1000]\n",
      "cost at  370 th iteration is :  0.163835 [  10 1000]\n",
      "cost at  375 th iteration is :  0.155576 [  10 1000]\n",
      "cost at  380 th iteration is :  0.161168 [  10 1000]\n",
      "cost at  385 th iteration is :  0.157601 [  10 1000]\n",
      "cost at  390 th iteration is :  0.161583 [  10 1000]\n",
      "cost at  395 th iteration is :  0.162114 [  10 1000]\n",
      "cost at  400 th iteration is :  0.145299 [  10 1000]\n",
      "cost at  405 th iteration is :  0.144949 [  10 1000]\n",
      "cost at  410 th iteration is :  0.127077 [  10 1000]\n",
      "cost at  415 th iteration is :  0.137974 [  10 1000]\n",
      "cost at  420 th iteration is :  0.168347 [  10 1000]\n",
      "cost at  425 th iteration is :  0.151466 [  10 1000]\n",
      "cost at  430 th iteration is :  0.172407 [  10 1000]\n",
      "cost at  435 th iteration is :  0.122758 [  10 1000]\n",
      "cost at  440 th iteration is :  0.14287 [  10 1000]\n",
      "cost at  445 th iteration is :  0.106072 [  10 1000]\n",
      "cost at  450 th iteration is :  0.158289 [  10 1000]\n",
      "cost at  455 th iteration is :  0.142481 [  10 1000]\n",
      "cost at  460 th iteration is :  0.131922 [  10 1000]\n",
      "cost at  465 th iteration is :  0.0947239 [  10 1000]\n",
      "cost at  470 th iteration is :  0.130835 [  10 1000]\n",
      "cost at  475 th iteration is :  0.0977145 [  10 1000]\n",
      "cost at  480 th iteration is :  0.12574 [  10 1000]\n",
      "cost at  485 th iteration is :  0.140024 [  10 1000]\n",
      "cost at  490 th iteration is :  0.123942 [  10 1000]\n",
      "cost at  495 th iteration is :  0.105095 [  10 1000]\n",
      "cost at  500 th iteration is :  0.109431 [  10 1000]\n",
      "cost at  505 th iteration is :  0.147618 [  10 1000]\n",
      "cost at  510 th iteration is :  0.100805 [  10 1000]\n",
      "cost at  515 th iteration is :  0.124656 [  10 1000]\n",
      "cost at  520 th iteration is :  0.105731 [  10 1000]\n",
      "cost at  525 th iteration is :  0.120093 [  10 1000]\n",
      "cost at  530 th iteration is :  0.133502 [  10 1000]\n",
      "cost at  535 th iteration is :  0.1335 [  10 1000]\n",
      "cost at  540 th iteration is :  0.127496 [  10 1000]\n",
      "cost at  545 th iteration is :  0.100386 [  10 1000]\n",
      "cost at  550 th iteration is :  0.106405 [  10 1000]\n",
      "cost at  555 th iteration is :  0.0790464 [  10 1000]\n",
      "cost at  560 th iteration is :  0.0993931 [  10 1000]\n",
      "cost at  565 th iteration is :  0.097108 [  10 1000]\n",
      "cost at  570 th iteration is :  0.123448 [  10 1000]\n",
      "cost at  575 th iteration is :  0.101385 [  10 1000]\n",
      "cost at  580 th iteration is :  0.103506 [  10 1000]\n",
      "cost at  585 th iteration is :  0.110481 [  10 1000]\n",
      "cost at  590 th iteration is :  0.0985494 [  10 1000]\n",
      "cost at  595 th iteration is :  0.0791282 [  10 1000]\n",
      "cost at  600 th iteration is :  0.0869896 [  10 1000]\n",
      "cost at  605 th iteration is :  0.0886897 [  10 1000]\n",
      "cost at  610 th iteration is :  0.105519 [  10 1000]\n",
      "cost at  615 th iteration is :  0.110863 [  10 1000]\n",
      "cost at  620 th iteration is :  0.105621 [  10 1000]\n",
      "cost at  625 th iteration is :  0.0788044 [  10 1000]\n",
      "cost at  630 th iteration is :  0.0926786 [  10 1000]\n",
      "cost at  635 th iteration is :  0.103712 [  10 1000]\n",
      "cost at  640 th iteration is :  0.0639942 [  10 1000]\n",
      "cost at  645 th iteration is :  0.0802298 [  10 1000]\n",
      "cost at  650 th iteration is :  0.0994088 [  10 1000]\n",
      "cost at  655 th iteration is :  0.0653906 [  10 1000]\n",
      "cost at  660 th iteration is :  0.0821438 [  10 1000]\n",
      "cost at  665 th iteration is :  0.0797111 [  10 1000]\n",
      "cost at  670 th iteration is :  0.093393 [  10 1000]\n",
      "cost at  675 th iteration is :  0.0883616 [  10 1000]\n",
      "cost at  680 th iteration is :  0.0777837 [  10 1000]\n",
      "cost at  685 th iteration is :  0.0890319 [  10 1000]\n",
      "cost at  690 th iteration is :  0.0817145 [  10 1000]\n",
      "cost at  695 th iteration is :  0.0731492 [  10 1000]\n",
      "cost at  700 th iteration is :  0.098634 [  10 1000]\n",
      "cost at  705 th iteration is :  0.0850115 [  10 1000]\n",
      "cost at  710 th iteration is :  0.0802668 [  10 1000]\n",
      "cost at  715 th iteration is :  0.0867605 [  10 1000]\n",
      "cost at  720 th iteration is :  0.0801135 [  10 1000]\n",
      "cost at  725 th iteration is :  0.0784297 [  10 1000]\n",
      "cost at  730 th iteration is :  0.071126 [  10 1000]\n",
      "cost at  735 th iteration is :  0.0686249 [  10 1000]\n",
      "cost at  740 th iteration is :  0.0780756 [  10 1000]\n",
      "cost at  745 th iteration is :  0.0705883 [  10 1000]\n",
      "cost at  750 th iteration is :  0.0738923 [  10 1000]\n",
      "cost at  755 th iteration is :  0.0762683 [  10 1000]\n",
      "cost at  760 th iteration is :  0.0922112 [  10 1000]\n",
      "cost at  765 th iteration is :  0.0694234 [  10 1000]\n",
      "cost at  770 th iteration is :  0.0528796 [  10 1000]\n",
      "cost at  775 th iteration is :  0.0627401 [  10 1000]\n",
      "cost at  780 th iteration is :  0.0641142 [  10 1000]\n",
      "cost at  785 th iteration is :  0.0879126 [  10 1000]\n",
      "cost at  790 th iteration is :  0.0562994 [  10 1000]\n",
      "cost at  795 th iteration is :  0.0681281 [  10 1000]\n",
      "cost at  800 th iteration is :  0.070876 [  10 1000]\n",
      "cost at  805 th iteration is :  0.0785224 [  10 1000]\n",
      "cost at  810 th iteration is :  0.0570577 [  10 1000]\n",
      "cost at  815 th iteration is :  0.072739 [  10 1000]\n",
      "cost at  820 th iteration is :  0.0725629 [  10 1000]\n",
      "cost at  825 th iteration is :  0.0610169 [  10 1000]\n",
      "cost at  830 th iteration is :  0.0751185 [  10 1000]\n",
      "cost at  835 th iteration is :  0.0733185 [  10 1000]\n",
      "cost at  840 th iteration is :  0.0695344 [  10 1000]\n",
      "cost at  845 th iteration is :  0.0604123 [  10 1000]\n",
      "cost at  850 th iteration is :  0.0795439 [  10 1000]\n",
      "cost at  855 th iteration is :  0.0671236 [  10 1000]\n",
      "cost at  860 th iteration is :  0.0497166 [  10 1000]\n",
      "cost at  865 th iteration is :  0.0659615 [  10 1000]\n",
      "cost at  870 th iteration is :  0.0844923 [  10 1000]\n",
      "cost at  875 th iteration is :  0.0677252 [  10 1000]\n",
      "cost at  880 th iteration is :  0.0573394 [  10 1000]\n",
      "cost at  885 th iteration is :  0.0543813 [  10 1000]\n",
      "cost at  890 th iteration is :  0.065887 [  10 1000]\n",
      "cost at  895 th iteration is :  0.0629423 [  10 1000]\n",
      "cost at  900 th iteration is :  0.0529562 [  10 1000]\n",
      "cost at  905 th iteration is :  0.0600781 [  10 1000]\n",
      "cost at  910 th iteration is :  0.0625675 [  10 1000]\n",
      "cost at  915 th iteration is :  0.0632069 [  10 1000]\n",
      "cost at  920 th iteration is :  0.0626132 [  10 1000]\n",
      "cost at  925 th iteration is :  0.0515993 [  10 1000]\n",
      "cost at  930 th iteration is :  0.0557744 [  10 1000]\n",
      "cost at  935 th iteration is :  0.0646192 [  10 1000]\n",
      "cost at  940 th iteration is :  0.0704311 [  10 1000]\n",
      "cost at  945 th iteration is :  0.0484786 [  10 1000]\n",
      "cost at  950 th iteration is :  0.0563379 [  10 1000]\n",
      "cost at  955 th iteration is :  0.0534566 [  10 1000]\n",
      "cost at  960 th iteration is :  0.0463282 [  10 1000]\n",
      "cost at  965 th iteration is :  0.053808 [  10 1000]\n",
      "cost at  970 th iteration is :  0.0388627 [  10 1000]\n",
      "cost at  975 th iteration is :  0.0557683 [  10 1000]\n",
      "cost at  980 th iteration is :  0.0495637 [  10 1000]\n",
      "cost at  985 th iteration is :  0.0502273 [  10 1000]\n",
      "cost at  990 th iteration is :  0.057104 [  10 1000]\n",
      "cost at  995 th iteration is :  0.0517611 [  10 1000]\n",
      "cost at  1000 th iteration is :  0.0542973 [  10 1000]\n",
      "cost at  1005 th iteration is :  0.0391067 [  10 1000]\n",
      "cost at  1010 th iteration is :  0.0512917 [  10 1000]\n",
      "cost at  1015 th iteration is :  0.0417253 [  10 1000]\n",
      "cost at  1020 th iteration is :  0.0527202 [  10 1000]\n",
      "cost at  1025 th iteration is :  0.0435255 [  10 1000]\n",
      "cost at  1030 th iteration is :  0.0545319 [  10 1000]\n",
      "cost at  1035 th iteration is :  0.062218 [  10 1000]\n",
      "cost at  1040 th iteration is :  0.0662887 [  10 1000]\n",
      "cost at  1045 th iteration is :  0.0470226 [  10 1000]\n",
      "cost at  1050 th iteration is :  0.0543408 [  10 1000]\n",
      "cost at  1055 th iteration is :  0.0546867 [  10 1000]\n",
      "cost at  1060 th iteration is :  0.0507403 [  10 1000]\n",
      "cost at  1065 th iteration is :  0.0395142 [  10 1000]\n",
      "cost at  1070 th iteration is :  0.0584586 [  10 1000]\n",
      "cost at  1075 th iteration is :  0.0521212 [  10 1000]\n",
      "cost at  1080 th iteration is :  0.0428208 [  10 1000]\n",
      "cost at  1085 th iteration is :  0.0500833 [  10 1000]\n",
      "cost at  1090 th iteration is :  0.0275837 [  10 1000]\n",
      "cost at  1095 th iteration is :  0.0449626 [  10 1000]\n",
      "cost at  1100 th iteration is :  0.0515507 [  10 1000]\n",
      "cost at  1105 th iteration is :  0.0477367 [  10 1000]\n",
      "cost at  1110 th iteration is :  0.0312427 [  10 1000]\n",
      "cost at  1115 th iteration is :  0.0399234 [  10 1000]\n",
      "cost at  1120 th iteration is :  0.0385961 [  10 1000]\n",
      "cost at  1125 th iteration is :  0.0386927 [  10 1000]\n",
      "cost at  1130 th iteration is :  0.0513456 [  10 1000]\n",
      "cost at  1135 th iteration is :  0.0504811 [  10 1000]\n",
      "cost at  1140 th iteration is :  0.0414031 [  10 1000]\n",
      "cost at  1145 th iteration is :  0.0423548 [  10 1000]\n",
      "cost at  1150 th iteration is :  0.041107 [  10 1000]\n",
      "cost at  1155 th iteration is :  0.0393369 [  10 1000]\n",
      "cost at  1160 th iteration is :  0.0359755 [  10 1000]\n",
      "cost at  1165 th iteration is :  0.0356573 [  10 1000]\n",
      "cost at  1170 th iteration is :  0.0505269 [  10 1000]\n",
      "cost at  1175 th iteration is :  0.0418359 [  10 1000]\n",
      "cost at  1180 th iteration is :  0.0537509 [  10 1000]\n",
      "cost at  1185 th iteration is :  0.0300753 [  10 1000]\n",
      "cost at  1190 th iteration is :  0.0454034 [  10 1000]\n",
      "cost at  1195 th iteration is :  0.0412917 [  10 1000]\n",
      "cost at  1200 th iteration is :  0.0404067 [  10 1000]\n",
      "cost at  1205 th iteration is :  0.0315635 [  10 1000]\n",
      "cost at  1210 th iteration is :  0.0316177 [  10 1000]\n",
      "cost at  1215 th iteration is :  0.0430035 [  10 1000]\n",
      "cost at  1220 th iteration is :  0.0324897 [  10 1000]\n",
      "cost at  1225 th iteration is :  0.0364225 [  10 1000]\n",
      "cost at  1230 th iteration is :  0.0351247 [  10 1000]\n",
      "cost at  1235 th iteration is :  0.0352504 [  10 1000]\n",
      "cost at  1240 th iteration is :  0.0320861 [  10 1000]\n",
      "cost at  1245 th iteration is :  0.0519503 [  10 1000]\n",
      "cost at  1250 th iteration is :  0.0335083 [  10 1000]\n",
      "cost at  1255 th iteration is :  0.0460875 [  10 1000]\n",
      "cost at  1260 th iteration is :  0.0398696 [  10 1000]\n",
      "cost at  1265 th iteration is :  0.0392661 [  10 1000]\n",
      "cost at  1270 th iteration is :  0.0369665 [  10 1000]\n",
      "cost at  1275 th iteration is :  0.0412046 [  10 1000]\n",
      "cost at  1280 th iteration is :  0.0383985 [  10 1000]\n",
      "cost at  1285 th iteration is :  0.0372524 [  10 1000]\n",
      "cost at  1290 th iteration is :  0.0386754 [  10 1000]\n",
      "cost at  1295 th iteration is :  0.0310802 [  10 1000]\n",
      "cost at  1300 th iteration is :  0.0403883 [  10 1000]\n",
      "cost at  1305 th iteration is :  0.0380484 [  10 1000]\n",
      "cost at  1310 th iteration is :  0.0307384 [  10 1000]\n",
      "cost at  1315 th iteration is :  0.0350975 [  10 1000]\n",
      "cost at  1320 th iteration is :  0.0345849 [  10 1000]\n",
      "cost at  1325 th iteration is :  0.028735 [  10 1000]\n",
      "cost at  1330 th iteration is :  0.0338854 [  10 1000]\n",
      "cost at  1335 th iteration is :  0.03631 [  10 1000]\n",
      "cost at  1340 th iteration is :  0.0360117 [  10 1000]\n",
      "cost at  1345 th iteration is :  0.0350192 [  10 1000]\n",
      "cost at  1350 th iteration is :  0.0350512 [  10 1000]\n",
      "cost at  1355 th iteration is :  0.038094 [  10 1000]\n",
      "cost at  1360 th iteration is :  0.0356331 [  10 1000]\n",
      "cost at  1365 th iteration is :  0.0357657 [  10 1000]\n",
      "cost at  1370 th iteration is :  0.0388786 [  10 1000]\n",
      "cost at  1375 th iteration is :  0.0278398 [  10 1000]\n",
      "cost at  1380 th iteration is :  0.0381044 [  10 1000]\n",
      "cost at  1385 th iteration is :  0.0316438 [  10 1000]\n",
      "cost at  1390 th iteration is :  0.0292284 [  10 1000]\n",
      "cost at  1395 th iteration is :  0.0476301 [  10 1000]\n",
      "cost at  1400 th iteration is :  0.0339488 [  10 1000]\n",
      "cost at  1405 th iteration is :  0.0252986 [  10 1000]\n",
      "cost at  1410 th iteration is :  0.034089 [  10 1000]\n",
      "cost at  1415 th iteration is :  0.0385508 [  10 1000]\n",
      "cost at  1420 th iteration is :  0.031371 [  10 1000]\n",
      "cost at  1425 th iteration is :  0.0200896 [  10 1000]\n",
      "cost at  1430 th iteration is :  0.0307799 [  10 1000]\n",
      "cost at  1435 th iteration is :  0.0334127 [  10 1000]\n",
      "cost at  1440 th iteration is :  0.0373425 [  10 1000]\n",
      "cost at  1445 th iteration is :  0.0315007 [  10 1000]\n",
      "cost at  1450 th iteration is :  0.0246445 [  10 1000]\n",
      "cost at  1455 th iteration is :  0.0310307 [  10 1000]\n",
      "cost at  1460 th iteration is :  0.0199226 [  10 1000]\n",
      "cost at  1465 th iteration is :  0.0325188 [  10 1000]\n",
      "cost at  1470 th iteration is :  0.0298944 [  10 1000]\n",
      "cost at  1475 th iteration is :  0.0329042 [  10 1000]\n",
      "cost at  1480 th iteration is :  0.0186651 [  10 1000]\n",
      "cost at  1485 th iteration is :  0.0292302 [  10 1000]\n",
      "cost at  1490 th iteration is :  0.0224379 [  10 1000]\n",
      "cost at  1495 th iteration is :  0.0219726 [  10 1000]\n",
      "cost at  1500 th iteration is :  0.0277459 [  10 1000]\n",
      "cost at  1505 th iteration is :  0.02977 [  10 1000]\n",
      "cost at  1510 th iteration is :  0.022004 [  10 1000]\n",
      "cost at  1515 th iteration is :  0.0226374 [  10 1000]\n",
      "cost at  1520 th iteration is :  0.0239908 [  10 1000]\n",
      "cost at  1525 th iteration is :  0.0354114 [  10 1000]\n",
      "cost at  1530 th iteration is :  0.0251297 [  10 1000]\n",
      "cost at  1535 th iteration is :  0.0240476 [  10 1000]\n",
      "cost at  1540 th iteration is :  0.023096 [  10 1000]\n",
      "cost at  1545 th iteration is :  0.0327559 [  10 1000]\n",
      "cost at  1550 th iteration is :  0.0271139 [  10 1000]\n",
      "cost at  1555 th iteration is :  0.0239414 [  10 1000]\n",
      "cost at  1560 th iteration is :  0.0216237 [  10 1000]\n",
      "cost at  1565 th iteration is :  0.0212228 [  10 1000]\n",
      "cost at  1570 th iteration is :  0.0295702 [  10 1000]\n",
      "cost at  1575 th iteration is :  0.0219053 [  10 1000]\n",
      "cost at  1580 th iteration is :  0.0255723 [  10 1000]\n",
      "cost at  1585 th iteration is :  0.0221148 [  10 1000]\n",
      "cost at  1590 th iteration is :  0.0297938 [  10 1000]\n",
      "cost at  1595 th iteration is :  0.0244309 [  10 1000]\n",
      "cost at  1600 th iteration is :  0.0218614 [  10 1000]\n",
      "cost at  1605 th iteration is :  0.0340449 [  10 1000]\n",
      "cost at  1610 th iteration is :  0.0244753 [  10 1000]\n",
      "cost at  1615 th iteration is :  0.0249475 [  10 1000]\n",
      "cost at  1620 th iteration is :  0.0328615 [  10 1000]\n",
      "cost at  1625 th iteration is :  0.0287634 [  10 1000]\n",
      "cost at  1630 th iteration is :  0.0258264 [  10 1000]\n",
      "cost at  1635 th iteration is :  0.0145871 [  10 1000]\n",
      "cost at  1640 th iteration is :  0.0307035 [  10 1000]\n",
      "cost at  1645 th iteration is :  0.0272275 [  10 1000]\n",
      "cost at  1650 th iteration is :  0.0274782 [  10 1000]\n",
      "cost at  1655 th iteration is :  0.0270406 [  10 1000]\n",
      "cost at  1660 th iteration is :  0.021131 [  10 1000]\n",
      "cost at  1665 th iteration is :  0.0221928 [  10 1000]\n",
      "cost at  1670 th iteration is :  0.0194468 [  10 1000]\n",
      "cost at  1675 th iteration is :  0.0267058 [  10 1000]\n",
      "cost at  1680 th iteration is :  0.0224724 [  10 1000]\n",
      "cost at  1685 th iteration is :  0.0321222 [  10 1000]\n",
      "cost at  1690 th iteration is :  0.0277156 [  10 1000]\n",
      "cost at  1695 th iteration is :  0.0253646 [  10 1000]\n",
      "cost at  1700 th iteration is :  0.0195488 [  10 1000]\n",
      "cost at  1705 th iteration is :  0.0205245 [  10 1000]\n",
      "cost at  1710 th iteration is :  0.0174031 [  10 1000]\n",
      "cost at  1715 th iteration is :  0.0271437 [  10 1000]\n",
      "cost at  1720 th iteration is :  0.0191036 [  10 1000]\n",
      "cost at  1725 th iteration is :  0.0241477 [  10 1000]\n",
      "cost at  1730 th iteration is :  0.0202799 [  10 1000]\n",
      "cost at  1735 th iteration is :  0.0212871 [  10 1000]\n",
      "cost at  1740 th iteration is :  0.0230737 [  10 1000]\n",
      "cost at  1745 th iteration is :  0.032257 [  10 1000]\n",
      "cost at  1750 th iteration is :  0.0236829 [  10 1000]\n",
      "cost at  1755 th iteration is :  0.017053 [  10 1000]\n",
      "cost at  1760 th iteration is :  0.0270503 [  10 1000]\n",
      "cost at  1765 th iteration is :  0.0205277 [  10 1000]\n",
      "cost at  1770 th iteration is :  0.014139 [  10 1000]\n",
      "cost at  1775 th iteration is :  0.0247935 [  10 1000]\n",
      "cost at  1780 th iteration is :  0.0217814 [  10 1000]\n",
      "cost at  1785 th iteration is :  0.0250897 [  10 1000]\n",
      "cost at  1790 th iteration is :  0.0198906 [  10 1000]\n",
      "cost at  1795 th iteration is :  0.0211895 [  10 1000]\n",
      "cost at  1800 th iteration is :  0.0213508 [  10 1000]\n",
      "cost at  1805 th iteration is :  0.0273 [  10 1000]\n",
      "cost at  1810 th iteration is :  0.0260018 [  10 1000]\n",
      "cost at  1815 th iteration is :  0.0249076 [  10 1000]\n",
      "cost at  1820 th iteration is :  0.0157064 [  10 1000]\n",
      "cost at  1825 th iteration is :  0.0270526 [  10 1000]\n",
      "cost at  1830 th iteration is :  0.0202321 [  10 1000]\n",
      "cost at  1835 th iteration is :  0.0242938 [  10 1000]\n",
      "cost at  1840 th iteration is :  0.0186522 [  10 1000]\n",
      "cost at  1845 th iteration is :  0.0175062 [  10 1000]\n",
      "cost at  1850 th iteration is :  0.0234184 [  10 1000]\n",
      "cost at  1855 th iteration is :  0.0215404 [  10 1000]\n",
      "cost at  1860 th iteration is :  0.0199663 [  10 1000]\n",
      "cost at  1865 th iteration is :  0.0239316 [  10 1000]\n",
      "cost at  1870 th iteration is :  0.0207272 [  10 1000]\n",
      "cost at  1875 th iteration is :  0.0159613 [  10 1000]\n",
      "cost at  1880 th iteration is :  0.0195827 [  10 1000]\n",
      "cost at  1885 th iteration is :  0.016794 [  10 1000]\n",
      "cost at  1890 th iteration is :  0.0182772 [  10 1000]\n",
      "cost at  1895 th iteration is :  0.0220245 [  10 1000]\n",
      "cost at  1900 th iteration is :  0.0187544 [  10 1000]\n",
      "cost at  1905 th iteration is :  0.0220953 [  10 1000]\n",
      "cost at  1910 th iteration is :  0.0265308 [  10 1000]\n",
      "cost at  1915 th iteration is :  0.0262331 [  10 1000]\n",
      "cost at  1920 th iteration is :  0.0226251 [  10 1000]\n",
      "cost at  1925 th iteration is :  0.0203401 [  10 1000]\n",
      "cost at  1930 th iteration is :  0.0137036 [  10 1000]\n",
      "cost at  1935 th iteration is :  0.017104 [  10 1000]\n",
      "cost at  1940 th iteration is :  0.0181865 [  10 1000]\n",
      "cost at  1945 th iteration is :  0.0186407 [  10 1000]\n",
      "cost at  1950 th iteration is :  0.0233123 [  10 1000]\n",
      "cost at  1955 th iteration is :  0.0240995 [  10 1000]\n",
      "cost at  1960 th iteration is :  0.0162057 [  10 1000]\n",
      "cost at  1965 th iteration is :  0.014365 [  10 1000]\n",
      "cost at  1970 th iteration is :  0.0233323 [  10 1000]\n",
      "cost at  1975 th iteration is :  0.0152123 [  10 1000]\n",
      "cost at  1980 th iteration is :  0.0173056 [  10 1000]\n",
      "cost at  1985 th iteration is :  0.013554 [  10 1000]\n",
      "cost at  1990 th iteration is :  0.0229049 [  10 1000]\n",
      "cost at  1995 th iteration is :  0.019457 [  10 1000]\n",
      "cost at  2000 th iteration is :  0.0137161 [  10 1000]\n",
      "cost at  2005 th iteration is :  0.0155321 [  10 1000]\n",
      "cost at  2010 th iteration is :  0.0210966 [  10 1000]\n",
      "cost at  2015 th iteration is :  0.0171676 [  10 1000]\n",
      "cost at  2020 th iteration is :  0.0178086 [  10 1000]\n",
      "cost at  2025 th iteration is :  0.0173662 [  10 1000]\n",
      "cost at  2030 th iteration is :  0.021974 [  10 1000]\n",
      "cost at  2035 th iteration is :  0.0181532 [  10 1000]\n",
      "cost at  2040 th iteration is :  0.0160848 [  10 1000]\n",
      "cost at  2045 th iteration is :  0.0175146 [  10 1000]\n",
      "cost at  2050 th iteration is :  0.0161953 [  10 1000]\n",
      "cost at  2055 th iteration is :  0.0159663 [  10 1000]\n",
      "cost at  2060 th iteration is :  0.0162207 [  10 1000]\n",
      "cost at  2065 th iteration is :  0.0171044 [  10 1000]\n",
      "cost at  2070 th iteration is :  0.0197943 [  10 1000]\n",
      "cost at  2075 th iteration is :  0.0166167 [  10 1000]\n",
      "cost at  2080 th iteration is :  0.0240773 [  10 1000]\n",
      "cost at  2085 th iteration is :  0.0251807 [  10 1000]\n",
      "cost at  2090 th iteration is :  0.0135825 [  10 1000]\n",
      "cost at  2095 th iteration is :  0.0237094 [  10 1000]\n",
      "cost at  2100 th iteration is :  0.0135253 [  10 1000]\n",
      "cost at  2105 th iteration is :  0.0181544 [  10 1000]\n",
      "cost at  2110 th iteration is :  0.0185283 [  10 1000]\n",
      "cost at  2115 th iteration is :  0.016226 [  10 1000]\n",
      "cost at  2120 th iteration is :  0.0187642 [  10 1000]\n",
      "cost at  2125 th iteration is :  0.0160679 [  10 1000]\n",
      "cost at  2130 th iteration is :  0.0187239 [  10 1000]\n",
      "cost at  2135 th iteration is :  0.0139385 [  10 1000]\n",
      "cost at  2140 th iteration is :  0.0152611 [  10 1000]\n",
      "cost at  2145 th iteration is :  0.0135451 [  10 1000]\n",
      "cost at  2150 th iteration is :  0.0166546 [  10 1000]\n",
      "cost at  2155 th iteration is :  0.0156924 [  10 1000]\n",
      "cost at  2160 th iteration is :  0.016421 [  10 1000]\n",
      "cost at  2165 th iteration is :  0.0152414 [  10 1000]\n",
      "cost at  2170 th iteration is :  0.0209722 [  10 1000]\n",
      "cost at  2175 th iteration is :  0.0134449 [  10 1000]\n",
      "cost at  2180 th iteration is :  0.0161879 [  10 1000]\n",
      "cost at  2185 th iteration is :  0.0167778 [  10 1000]\n",
      "cost at  2190 th iteration is :  0.0173532 [  10 1000]\n",
      "cost at  2195 th iteration is :  0.012952 [  10 1000]\n",
      "cost at  2200 th iteration is :  0.0130647 [  10 1000]\n",
      "cost at  2205 th iteration is :  0.0109411 [  10 1000]\n",
      "cost at  2210 th iteration is :  0.0137901 [  10 1000]\n",
      "cost at  2215 th iteration is :  0.0170197 [  10 1000]\n",
      "cost at  2220 th iteration is :  0.0141875 [  10 1000]\n",
      "cost at  2225 th iteration is :  0.0184531 [  10 1000]\n",
      "cost at  2230 th iteration is :  0.0165507 [  10 1000]\n",
      "cost at  2235 th iteration is :  0.0196565 [  10 1000]\n",
      "cost at  2240 th iteration is :  0.0153908 [  10 1000]\n",
      "cost at  2245 th iteration is :  0.0170586 [  10 1000]\n",
      "cost at  2250 th iteration is :  0.011494 [  10 1000]\n",
      "cost at  2255 th iteration is :  0.0105956 [  10 1000]\n",
      "cost at  2260 th iteration is :  0.0108461 [  10 1000]\n",
      "cost at  2265 th iteration is :  0.0162982 [  10 1000]\n",
      "cost at  2270 th iteration is :  0.0158928 [  10 1000]\n",
      "cost at  2275 th iteration is :  0.0177627 [  10 1000]\n",
      "cost at  2280 th iteration is :  0.0162563 [  10 1000]\n",
      "cost at  2285 th iteration is :  0.0171256 [  10 1000]\n",
      "cost at  2290 th iteration is :  0.0137445 [  10 1000]\n",
      "cost at  2295 th iteration is :  0.0158817 [  10 1000]\n",
      "cost at  2300 th iteration is :  0.0157151 [  10 1000]\n",
      "cost at  2305 th iteration is :  0.011337 [  10 1000]\n",
      "cost at  2310 th iteration is :  0.0124603 [  10 1000]\n",
      "cost at  2315 th iteration is :  0.0114598 [  10 1000]\n",
      "cost at  2320 th iteration is :  0.0173109 [  10 1000]\n",
      "cost at  2325 th iteration is :  0.012012 [  10 1000]\n",
      "cost at  2330 th iteration is :  0.0128544 [  10 1000]\n",
      "cost at  2335 th iteration is :  0.0147822 [  10 1000]\n",
      "cost at  2340 th iteration is :  0.013065 [  10 1000]\n",
      "cost at  2345 th iteration is :  0.0142762 [  10 1000]\n",
      "cost at  2350 th iteration is :  0.013374 [  10 1000]\n",
      "cost at  2355 th iteration is :  0.0168644 [  10 1000]\n",
      "cost at  2360 th iteration is :  0.0131088 [  10 1000]\n",
      "cost at  2365 th iteration is :  0.00914853 [  10 1000]\n",
      "cost at  2370 th iteration is :  0.0150567 [  10 1000]\n",
      "cost at  2375 th iteration is :  0.0158329 [  10 1000]\n",
      "cost at  2380 th iteration is :  0.0143954 [  10 1000]\n",
      "cost at  2385 th iteration is :  0.00985565 [  10 1000]\n",
      "cost at  2390 th iteration is :  0.0169901 [  10 1000]\n",
      "cost at  2395 th iteration is :  0.0122403 [  10 1000]\n",
      "cost at  2400 th iteration is :  0.0150001 [  10 1000]\n",
      "cost at  2405 th iteration is :  0.016453 [  10 1000]\n",
      "cost at  2410 th iteration is :  0.013799 [  10 1000]\n",
      "cost at  2415 th iteration is :  0.0119072 [  10 1000]\n",
      "cost at  2420 th iteration is :  0.012721 [  10 1000]\n",
      "cost at  2425 th iteration is :  0.0101719 [  10 1000]\n",
      "cost at  2430 th iteration is :  0.013862 [  10 1000]\n",
      "cost at  2435 th iteration is :  0.0139002 [  10 1000]\n",
      "cost at  2440 th iteration is :  0.0101447 [  10 1000]\n",
      "cost at  2445 th iteration is :  0.0170878 [  10 1000]\n",
      "cost at  2450 th iteration is :  0.0101927 [  10 1000]\n",
      "cost at  2455 th iteration is :  0.0131309 [  10 1000]\n",
      "cost at  2460 th iteration is :  0.0138297 [  10 1000]\n",
      "cost at  2465 th iteration is :  0.00980187 [  10 1000]\n",
      "cost at  2470 th iteration is :  0.012169 [  10 1000]\n",
      "cost at  2475 th iteration is :  0.0103409 [  10 1000]\n",
      "cost at  2480 th iteration is :  0.0132954 [  10 1000]\n",
      "cost at  2485 th iteration is :  0.00976069 [  10 1000]\n",
      "cost at  2490 th iteration is :  0.0102229 [  10 1000]\n",
      "cost at  2495 th iteration is :  0.0148507 [  10 1000]\n",
      "cost at  2500 th iteration is :  0.0139088 [  10 1000]\n",
      "cost at  2505 th iteration is :  0.0123587 [  10 1000]\n",
      "cost at  2510 th iteration is :  0.0141 [  10 1000]\n",
      "cost at  2515 th iteration is :  0.0127153 [  10 1000]\n",
      "cost at  2520 th iteration is :  0.0207425 [  10 1000]\n",
      "cost at  2525 th iteration is :  0.011151 [  10 1000]\n",
      "cost at  2530 th iteration is :  0.00849098 [  10 1000]\n",
      "cost at  2535 th iteration is :  0.0130469 [  10 1000]\n",
      "cost at  2540 th iteration is :  0.0135107 [  10 1000]\n",
      "cost at  2545 th iteration is :  0.019169 [  10 1000]\n",
      "cost at  2550 th iteration is :  0.0110754 [  10 1000]\n",
      "cost at  2555 th iteration is :  0.0109349 [  10 1000]\n",
      "cost at  2560 th iteration is :  0.0132142 [  10 1000]\n",
      "cost at  2565 th iteration is :  0.0124513 [  10 1000]\n",
      "cost at  2570 th iteration is :  0.0101171 [  10 1000]\n",
      "cost at  2575 th iteration is :  0.0141739 [  10 1000]\n",
      "cost at  2580 th iteration is :  0.0105875 [  10 1000]\n",
      "cost at  2585 th iteration is :  0.0129128 [  10 1000]\n",
      "cost at  2590 th iteration is :  0.00681807 [  10 1000]\n",
      "cost at  2595 th iteration is :  0.00924417 [  10 1000]\n",
      "cost at  2600 th iteration is :  0.0141305 [  10 1000]\n",
      "cost at  2605 th iteration is :  0.00775412 [  10 1000]\n",
      "cost at  2610 th iteration is :  0.0144441 [  10 1000]\n",
      "cost at  2615 th iteration is :  0.0122196 [  10 1000]\n",
      "cost at  2620 th iteration is :  0.00952133 [  10 1000]\n",
      "cost at  2625 th iteration is :  0.0120304 [  10 1000]\n",
      "cost at  2630 th iteration is :  0.0149416 [  10 1000]\n",
      "cost at  2635 th iteration is :  0.00861763 [  10 1000]\n",
      "cost at  2640 th iteration is :  0.0100374 [  10 1000]\n",
      "cost at  2645 th iteration is :  0.0129731 [  10 1000]\n",
      "cost at  2650 th iteration is :  0.0114174 [  10 1000]\n",
      "cost at  2655 th iteration is :  0.010697 [  10 1000]\n",
      "cost at  2660 th iteration is :  0.0133874 [  10 1000]\n",
      "cost at  2665 th iteration is :  0.0118752 [  10 1000]\n",
      "cost at  2670 th iteration is :  0.0135662 [  10 1000]\n",
      "cost at  2675 th iteration is :  0.0105025 [  10 1000]\n",
      "cost at  2680 th iteration is :  0.0132963 [  10 1000]\n",
      "cost at  2685 th iteration is :  0.0119983 [  10 1000]\n",
      "cost at  2690 th iteration is :  0.0108943 [  10 1000]\n",
      "cost at  2695 th iteration is :  0.0102054 [  10 1000]\n",
      "cost at  2700 th iteration is :  0.0117761 [  10 1000]\n",
      "cost at  2705 th iteration is :  0.0124096 [  10 1000]\n",
      "cost at  2710 th iteration is :  0.012595 [  10 1000]\n",
      "cost at  2715 th iteration is :  0.00955183 [  10 1000]\n",
      "cost at  2720 th iteration is :  0.00839886 [  10 1000]\n",
      "cost at  2725 th iteration is :  0.0144081 [  10 1000]\n",
      "cost at  2730 th iteration is :  0.0137315 [  10 1000]\n",
      "cost at  2735 th iteration is :  0.00825204 [  10 1000]\n",
      "cost at  2740 th iteration is :  0.0114033 [  10 1000]\n",
      "cost at  2745 th iteration is :  0.0101642 [  10 1000]\n",
      "cost at  2750 th iteration is :  0.0137715 [  10 1000]\n",
      "cost at  2755 th iteration is :  0.0119964 [  10 1000]\n",
      "cost at  2760 th iteration is :  0.0111607 [  10 1000]\n",
      "cost at  2765 th iteration is :  0.0109066 [  10 1000]\n",
      "cost at  2770 th iteration is :  0.00863687 [  10 1000]\n",
      "cost at  2775 th iteration is :  0.00546631 [  10 1000]\n",
      "cost at  2780 th iteration is :  0.00892964 [  10 1000]\n",
      "cost at  2785 th iteration is :  0.00961915 [  10 1000]\n",
      "cost at  2790 th iteration is :  0.00866476 [  10 1000]\n",
      "cost at  2795 th iteration is :  0.0151065 [  10 1000]\n",
      "cost at  2800 th iteration is :  0.0110693 [  10 1000]\n",
      "cost at  2805 th iteration is :  0.00892473 [  10 1000]\n",
      "cost at  2810 th iteration is :  0.00959985 [  10 1000]\n",
      "cost at  2815 th iteration is :  0.0101438 [  10 1000]\n",
      "cost at  2820 th iteration is :  0.00811714 [  10 1000]\n",
      "cost at  2825 th iteration is :  0.00835477 [  10 1000]\n",
      "cost at  2830 th iteration is :  0.0104888 [  10 1000]\n",
      "cost at  2835 th iteration is :  0.00851584 [  10 1000]\n",
      "cost at  2840 th iteration is :  0.0124158 [  10 1000]\n",
      "cost at  2845 th iteration is :  0.0130268 [  10 1000]\n",
      "cost at  2850 th iteration is :  0.0160374 [  10 1000]\n",
      "cost at  2855 th iteration is :  0.00731526 [  10 1000]\n",
      "cost at  2860 th iteration is :  0.0111495 [  10 1000]\n",
      "cost at  2865 th iteration is :  0.00911746 [  10 1000]\n",
      "cost at  2870 th iteration is :  0.0107698 [  10 1000]\n",
      "cost at  2875 th iteration is :  0.0109168 [  10 1000]\n",
      "cost at  2880 th iteration is :  0.00856571 [  10 1000]\n",
      "cost at  2885 th iteration is :  0.0116875 [  10 1000]\n",
      "cost at  2890 th iteration is :  0.010254 [  10 1000]\n",
      "cost at  2895 th iteration is :  0.0088393 [  10 1000]\n",
      "cost at  2900 th iteration is :  0.0127652 [  10 1000]\n",
      "cost at  2905 th iteration is :  0.0122858 [  10 1000]\n",
      "cost at  2910 th iteration is :  0.00981671 [  10 1000]\n",
      "cost at  2915 th iteration is :  0.00906631 [  10 1000]\n",
      "cost at  2920 th iteration is :  0.00774931 [  10 1000]\n",
      "cost at  2925 th iteration is :  0.00662761 [  10 1000]\n",
      "cost at  2930 th iteration is :  0.0057256 [  10 1000]\n",
      "cost at  2935 th iteration is :  0.00695937 [  10 1000]\n",
      "cost at  2940 th iteration is :  0.0100884 [  10 1000]\n",
      "cost at  2945 th iteration is :  0.00814463 [  10 1000]\n",
      "cost at  2950 th iteration is :  0.00797074 [  10 1000]\n",
      "cost at  2955 th iteration is :  0.0120684 [  10 1000]\n",
      "cost at  2960 th iteration is :  0.00896886 [  10 1000]\n",
      "cost at  2965 th iteration is :  0.00922538 [  10 1000]\n",
      "cost at  2970 th iteration is :  0.00834212 [  10 1000]\n",
      "cost at  2975 th iteration is :  0.00731234 [  10 1000]\n",
      "cost at  2980 th iteration is :  0.0128272 [  10 1000]\n",
      "cost at  2985 th iteration is :  0.0065829 [  10 1000]\n",
      "cost at  2990 th iteration is :  0.00825143 [  10 1000]\n",
      "cost at  2995 th iteration is :  0.00996528 [  10 1000]\n",
      "training accuracy :  0.633545\n",
      "testing accuracy :  0.5746\n",
      "This run of \"MNIST 2 layer NN\" ran for 0:02:51 and logs are available locally at: /Users/bhavul.g/.hyperdash/logs/mnist-2-layer-nn/mnist-2-layer-nn_2018-04-10t12-34-33-169931.log\n"
     ]
    }
   ],
   "source": [
    "%%monitor_cell \"MNIST 2 layer NN\"\n",
    "## 2 Layer NN\n",
    "\n",
    "# code to add\n",
    "X1 = tf.placeholder(tf.float32, [784, None], name='X1')\n",
    "Y1 = tf.placeholder(tf.float32, [10, None], name='Y1')\n",
    "Y1_cls = tf.argmax(Y1, axis=0)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal((100,784)))\n",
    "b1 = tf.Variable(tf.zeros((1,1)))           \n",
    "\n",
    "W2 = tf.Variable(tf.random_normal((10,100)))\n",
    "b2 = tf.Variable(tf.zeros((1,1)))\n",
    "\n",
    "Z1 = tf.matmul(W1,X1) + b1\n",
    "A1 = tf.nn.relu(Z1)\n",
    "\n",
    "Z2 = tf.matmul(W2,A1) + b2\n",
    "A2 = tf.nn.softmax(Z2)\n",
    "\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=Z2, labels=Y1)\n",
    "# Reduce cost to just one number. \n",
    "cost_mean = tf.reduce_mean(cost) \n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost_mean)\n",
    "\n",
    "# We'll do a softmax for this so that ever number comes b/w 0 and 1. Together they should sum up to 1. Normalization.\n",
    "Y2_pred_axis0 = tf.argmax(A2, axis=0)\n",
    "\n",
    "\n",
    "session = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "\n",
    "# mini-batches of 1000 (so 55 mini-batches)\n",
    "for i in range(3000):\n",
    "    X_batch, Y_batch = data.train.next_batch(1000)\n",
    "    _, cost_mean_val, shapeZ2,pred_axis0 = session.run([optimizer,cost_mean,tf.shape(Z2),Y2_pred_axis0], feed_dict={X1:X_batch.T, Y1:Y_batch.T})\n",
    "    if i%5 == 0:\n",
    "        print(\"cost at \",i,\"th iteration is : \",cost_mean_val,shapeZ2)\n",
    "\n",
    "correct_prediction = tf.equal(Y2_pred_axis0, Y1_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"training accuracy : \",session.run(accuracy, feed_dict={X1:data.train.images.T, Y1:data.train.labels.T}))\n",
    "print(\"testing accuracy : \",session.run(accuracy, feed_dict={X1:data.test.images.T, Y1:data.test.labels.T}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hh1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
